{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","authorship_tag":"ABX9TyOrvw3Ev6yjo2CVYxO34YSc"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","widgets":{"application/vnd.jupyter.widget-state+json":{"b53508245213403982ef496277ff5d15":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_7459b6e398d14e4489f212bf8341e2c8","IPY_MODEL_f8cdd1e058d44be2ab31cf6744cd7fd5","IPY_MODEL_e939e41aef3047129e9c9824f839583e"],"layout":"IPY_MODEL_2256da619d81435ea02f95f2c70f7011"}},"7459b6e398d14e4489f212bf8341e2c8":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_d7e539ef08d64c9088fe95a070ebf2e2","placeholder":"​","style":"IPY_MODEL_c1b9698374754135a9c1edd125d00a82","value":"Map: 100%"}},"f8cdd1e058d44be2ab31cf6744cd7fd5":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_002829287e4b44499a0c8f32a7b38733","max":228,"min":0,"orientation":"horizontal","style":"IPY_MODEL_1479fe0029e14bb382edc0b8853a0fc4","value":228}},"e939e41aef3047129e9c9824f839583e":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_d7961f2c52594affa9ce457c3624c9b9","placeholder":"​","style":"IPY_MODEL_26645bc2769b486aa27e202e36d45ccb","value":" 228/228 [02:52&lt;00:00,  1.47 examples/s]"}},"2256da619d81435ea02f95f2c70f7011":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d7e539ef08d64c9088fe95a070ebf2e2":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c1b9698374754135a9c1edd125d00a82":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"002829287e4b44499a0c8f32a7b38733":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1479fe0029e14bb382edc0b8853a0fc4":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"d7961f2c52594affa9ce457c3624c9b9":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"26645bc2769b486aa27e202e36d45ccb":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"7686ce04e2bc4866a17b283106ca6b6e":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_84fe58b7802b434ba386e21e7e70922f","IPY_MODEL_6b21e2fbebe34fe3b243579d6bb02023","IPY_MODEL_5e259548621d42d4a7a64d20d87ddfc2"],"layout":"IPY_MODEL_a2818bdd05b6459db14e678d26631a8d"}},"84fe58b7802b434ba386e21e7e70922f":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_8e3258298a284518a83aaf61791dd402","placeholder":"​","style":"IPY_MODEL_69f03bced9ec4c1da3d627f0590556e7","value":"Map: 100%"}},"6b21e2fbebe34fe3b243579d6bb02023":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_b47d25270958431489b11504a7b85676","max":34,"min":0,"orientation":"horizontal","style":"IPY_MODEL_d588172e592446929b7e1b9c082d31dd","value":34}},"5e259548621d42d4a7a64d20d87ddfc2":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_0807168686fb440aa82d513a404cbe80","placeholder":"​","style":"IPY_MODEL_0ca78265e49a4268a5dd21ab571ce215","value":" 34/34 [00:24&lt;00:00,  1.41 examples/s]"}},"a2818bdd05b6459db14e678d26631a8d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8e3258298a284518a83aaf61791dd402":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"69f03bced9ec4c1da3d627f0590556e7":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"b47d25270958431489b11504a7b85676":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d588172e592446929b7e1b9c082d31dd":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"0807168686fb440aa82d513a404cbe80":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0ca78265e49a4268a5dd21ab571ce215":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"a9qr0abTxO5c","executionInfo":{"status":"ok","timestamp":1762530683891,"user_tz":-330,"elapsed":4855,"user":{"displayName":"Prathamesh Shanbhag","userId":"16977587014656307330"}},"outputId":"abf87a5b-c891-4a2f-d5ea-2a3aad907b6d"},"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n","Drive mounted and dataset path set to: /content/drive/MyDrive/dataset\n"]}],"source":["# cell: mount_drive\n","from google.colab import drive\n","drive.mount('/content/drive')\n","\n","\n","# Path to your dataset folder on Google Drive\n","DATA_ROOT = '/content/drive/MyDrive/dataset'\n","print('Drive mounted and dataset path set to:', DATA_ROOT)"]},{"cell_type":"code","source":["# cell: install_deps\n","!pip install --upgrade pip\n","!pip install transformers==4.44.2 datasets accelerate evaluate ftfy regex sentencepiece pillow torchvision timm einops"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"bJyStnNlxzjv","executionInfo":{"status":"ok","timestamp":1762530707273,"user_tz":-330,"elapsed":23400,"user":{"displayName":"Prathamesh Shanbhag","userId":"16977587014656307330"}},"outputId":"7d948f6c-9b9c-461b-b0ff-adf763b32738"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: pip in /usr/local/lib/python3.12/dist-packages (25.3)\n","Requirement already satisfied: transformers==4.44.2 in /usr/local/lib/python3.12/dist-packages (4.44.2)\n","Requirement already satisfied: datasets in /usr/local/lib/python3.12/dist-packages (4.0.0)\n","Requirement already satisfied: accelerate in /usr/local/lib/python3.12/dist-packages (1.11.0)\n","Requirement already satisfied: evaluate in /usr/local/lib/python3.12/dist-packages (0.4.6)\n","Requirement already satisfied: ftfy in /usr/local/lib/python3.12/dist-packages (6.3.1)\n","Requirement already satisfied: regex in /usr/local/lib/python3.12/dist-packages (2024.11.6)\n","Requirement already satisfied: sentencepiece in /usr/local/lib/python3.12/dist-packages (0.2.1)\n","Requirement already satisfied: pillow in /usr/local/lib/python3.12/dist-packages (11.3.0)\n","Requirement already satisfied: torchvision in /usr/local/lib/python3.12/dist-packages (0.23.0+cu126)\n","Requirement already satisfied: timm in /usr/local/lib/python3.12/dist-packages (1.0.21)\n","Requirement already satisfied: einops in /usr/local/lib/python3.12/dist-packages (0.8.1)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from transformers==4.44.2) (3.20.0)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.12/dist-packages (from transformers==4.44.2) (0.36.0)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from transformers==4.44.2) (2.0.2)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from transformers==4.44.2) (25.0)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from transformers==4.44.2) (6.0.3)\n","Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from transformers==4.44.2) (2.32.4)\n","Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.12/dist-packages (from transformers==4.44.2) (0.6.2)\n","Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.12/dist-packages (from transformers==4.44.2) (0.19.1)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.12/dist-packages (from transformers==4.44.2) (4.67.1)\n","Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers==4.44.2) (2025.3.0)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers==4.44.2) (4.15.0)\n","Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers==4.44.2) (1.2.0)\n","Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (18.1.0)\n","Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.3.8)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from datasets) (2.2.2)\n","Requirement already satisfied: xxhash in /usr/local/lib/python3.12/dist-packages (from datasets) (3.6.0)\n","Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.70.16)\n","Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.13.2)\n","Requirement already satisfied: psutil in /usr/local/lib/python3.12/dist-packages (from accelerate) (5.9.5)\n","Requirement already satisfied: torch>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from accelerate) (2.8.0+cu126)\n","Requirement already satisfied: wcwidth in /usr/local/lib/python3.12/dist-packages (from ftfy) (0.2.14)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (75.2.0)\n","Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (1.13.3)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (3.5)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (3.1.6)\n","Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.6.77)\n","Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.6.77)\n","Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.6.80)\n","Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (9.10.2.21)\n","Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.6.4.1)\n","Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (11.3.0.4)\n","Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (10.3.7.77)\n","Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (11.7.1.2)\n","Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.5.4.2)\n","Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (0.7.1)\n","Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (2.27.3)\n","Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.6.77)\n","Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.6.85)\n","Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (1.11.1.6)\n","Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (3.4.0)\n","Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2.6.1)\n","Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.4.0)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (25.4.0)\n","Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.8.0)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (6.7.0)\n","Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (0.4.1)\n","Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.22.0)\n","Requirement already satisfied: idna>=2.0 in /usr/local/lib/python3.12/dist-packages (from yarl<2.0,>=1.17.0->aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.11)\n","Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->transformers==4.44.2) (3.4.4)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->transformers==4.44.2) (2.5.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->transformers==4.44.2) (2025.10.5)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=2.0.0->accelerate) (1.3.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=2.0.0->accelerate) (3.0.3)\n","Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2.9.0.post0)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2025.2)\n","Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2025.2)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n"]}]},{"cell_type":"code","source":["!apt-get -q install poppler-utils"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"vv7qYyTq2kJy","executionInfo":{"status":"ok","timestamp":1762530712143,"user_tz":-330,"elapsed":4823,"user":{"displayName":"Prathamesh Shanbhag","userId":"16977587014656307330"}},"outputId":"038e4d98-dee1-4149-e746-d58a0f840a1b"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Reading package lists...\n","Building dependency tree...\n","Reading state information...\n","poppler-utils is already the newest version (22.02.0-2ubuntu0.12).\n","0 upgraded, 0 newly installed, 0 to remove and 41 not upgraded.\n"]}]},{"cell_type":"code","source":["!pip install pdf2image"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"tBQZcAfZ2yCW","executionInfo":{"status":"ok","timestamp":1762530718278,"user_tz":-330,"elapsed":6083,"user":{"displayName":"Prathamesh Shanbhag","userId":"16977587014656307330"}},"outputId":"98efcdf9-b504-4e93-bd4c-58a17250f2b2"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: pdf2image in /usr/local/lib/python3.12/dist-packages (1.17.0)\n","Requirement already satisfied: pillow in /usr/local/lib/python3.12/dist-packages (from pdf2image) (11.3.0)\n"]}]},{"cell_type":"code","source":["import os\n","import json\n","import math\n","import random\n","import shutil\n","import glob\n","from pathlib import Path\n","from typing import List, Dict\n","\n","import torch\n","from PIL import Image,ImageFile\n","from pdf2image import convert_from_path\n","from tqdm.auto import tqdm\n","\n","ImageFile.LOAD_TRUNCATED_IMAGES = True\n","\n","from transformers import (\n","    AutoProcessor,\n","    VisionEncoderDecoderModel,\n","    TrainingArguments,\n","    Trainer,\n","    default_data_collator,\n",")\n","from datasets import Dataset\n","from peft import LoraConfig, get_peft_model, TaskType"],"metadata":{"id":"DcL8wgaGyHx4","executionInfo":{"status":"ok","timestamp":1762530749126,"user_tz":-330,"elapsed":30812,"user":{"displayName":"Prathamesh Shanbhag","userId":"16977587014656307330"}}},"execution_count":5,"outputs":[]},{"cell_type":"code","source":["# ---------------- CONFIG ----------------\n","GDRIVE_ROOT = \"/content/drive/MyDrive\"   # change if needed\n","DATASET_DRIVE_PATH = f\"{GDRIVE_ROOT}/dataset\"\n","OUTPUT_DIR = f\"{GDRIVE_ROOT}/donut_peft_lora_output\"  # where to save adapters and checkpoints\n","LOG_DIR = f\"{OUTPUT_DIR}/logs\"\n","print(\"DATASET_DRIVE_PATH =\", DATASET_DRIVE_PATH)\n","print(\"OUTPUT_DIR =\", OUTPUT_DIR)\n","MODEL_NAME = \"naver-clova-ix/donut-base-finetuned-cord-v2\"  # HF model to fine-tune\n","TASK_TAG = \"parse\"\n","\n","# Paths (modify if needed)\n","DATASET_ROOT = os.environ.get(\"DATASET_ROOT\", DATASET_DRIVE_PATH)\n","# expected:\n","# DATASET_ROOT/train/images/*\n","# DATASET_ROOT/train/metadata.jsonl\n","# DATASET_ROOT/val/images/*\n","# DATASET_ROOT/val/metadata.jsonl\n","\n","OUTPUT_DIR = os.environ.get(\"OUTPUT_DIR\",OUTPUT_DIR)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"OoKHUr5Dzw0W","executionInfo":{"status":"ok","timestamp":1762530749215,"user_tz":-330,"elapsed":26,"user":{"displayName":"Prathamesh Shanbhag","userId":"16977587014656307330"}},"outputId":"ebf90a4d-01d6-4751-80bb-e7a2e23003ec"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["DATASET_DRIVE_PATH = /content/drive/MyDrive/dataset\n","OUTPUT_DIR = /content/drive/MyDrive/donut_peft_lora_output\n"]}]},{"cell_type":"code","source":["NUM_EPOCHS = 25\n","TRAIN_BATCH_SIZE = 2\n","EVAL_BATCH_SIZE = 2\n","LEARNING_RATE = 5e-5\n","MAX_TARGET_LENGTH = 512\n","SAVE_STEPS = 100        # checkpoint frequency (steps)\n","SAVE_TOTAL_LIMIT = 5\n","SEED = 42\n","LOGGING_STEPS = 10\n","# LoRA config\n","LORA_R = 8\n","LORA_ALPHA = 32\n","LORA_DROPOUT = 0.05"],"metadata":{"id":"JTCo-S1Y6Bpm","executionInfo":{"status":"ok","timestamp":1762530749230,"user_tz":-330,"elapsed":8,"user":{"displayName":"Prathamesh Shanbhag","userId":"16977587014656307330"}}},"execution_count":7,"outputs":[]},{"cell_type":"code","source":["# PDF -> image conversion options\n","PDF_DPI = 200  # good tradeoff for invoices\n","\n","# Utility\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","os.makedirs(OUTPUT_DIR, exist_ok=True)\n","print(\"Device:\", device)\n","print(\"Output dir:\", OUTPUT_DIR)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"R6LKZ5_r0Zne","executionInfo":{"status":"ok","timestamp":1762530749267,"user_tz":-330,"elapsed":25,"user":{"displayName":"Prathamesh Shanbhag","userId":"16977587014656307330"}},"outputId":"b2136d0a-b1f2-458e-9ba1-962eff6ae8f1"},"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["Device: cuda\n","Output dir: /content/drive/MyDrive/donut_peft_lora_output\n"]}]},{"cell_type":"code","source":["# ---------------- helpers ----------------\n","def ensure_dir(p):\n","    os.makedirs(p, exist_ok=True)\n","\n","def read_jsonl(meta_path: str) -> List[Dict]:\n","    out = []\n","    with open(meta_path, \"r\", encoding=\"utf-8\") as f:\n","        for line in f:\n","            line=line.strip()\n","            if not line:\n","                continue\n","            out.append(json.loads(line))\n","    return out"],"metadata":{"id":"s1Tpb-4T0kFD","executionInfo":{"status":"ok","timestamp":1762530749277,"user_tz":-330,"elapsed":6,"user":{"displayName":"Prathamesh Shanbhag","userId":"16977587014656307330"}}},"execution_count":9,"outputs":[]},{"cell_type":"code","source":["def convert_pdf_to_images(pdf_path: str, out_folder: str, dpi:int = PDF_DPI) -> List[str]:\n","    \"\"\"\n","    Convert a PDF into PNG pages in out_folder.\n","    Return list of generated image file paths.\n","    \"\"\"\n","    ensure_dir(out_folder)\n","    pages = convert_from_path(pdf_path, dpi=dpi)\n","    out_paths = []\n","    base = Path(pdf_path).stem\n","    for i, page in enumerate(pages, start=1):\n","        out_name = f\"{base}_page_{i}.png\"\n","        out_path = os.path.join(out_folder, out_name)\n","        page.save(out_path, \"PNG\")\n","        out_paths.append(out_path)\n","    return out_paths"],"metadata":{"id":"Y7z-CAF51DJZ","executionInfo":{"status":"ok","timestamp":1762530749301,"user_tz":-330,"elapsed":20,"user":{"displayName":"Prathamesh Shanbhag","userId":"16977587014656307330"}}},"execution_count":10,"outputs":[]},{"cell_type":"code","source":["def prepare_files_and_records(root: str, split: str, tmp_processed_folder: str):\n","    \"\"\"\n","    For each file in split/images/ (png/jpg/jpeg/pdf), copy/convert into tmp_processed_folder/<split>/images/\n","    Return a list of dicts: {\"image\": \"<filename>\", \"image_path\": \"<abs path>\", \"ground_truth\": \"<wrapped JSON string>\"}\n","    \"\"\"\n","    split_img_folder = os.path.join(root, split, \"images\")\n","    meta_path = os.path.join(root, split, \"metadata.jsonl\")\n","    assert os.path.exists(split_img_folder), f\"{split_img_folder} not found\"\n","    assert os.path.exists(meta_path), f\"{meta_path} not found\"\n","\n","    metadata = read_jsonl(meta_path)\n","    # build dict for quick lookup by filename\n","    meta_by_name = { rec[\"image\"]: rec[\"ground_truth\"] for rec in metadata }\n","\n","    processed_images_dir = os.path.join(tmp_processed_folder, split, \"images\")\n","    ensure_dir(processed_images_dir)\n","\n","    records = []\n","\n","    for fname, gt in meta_by_name.items():\n","        src_path = os.path.join(split_img_folder, fname)\n","        if not os.path.exists(src_path):\n","            print(f\"⚠️ Warning: {src_path} not found. Skipping.\")\n","            continue\n","\n","        lower = fname.lower()\n","        if lower.endswith(\".pdf\"):\n","            # convert pages\n","            pages = convert_pdf_to_images(src_path, processed_images_dir)\n","            for p in pages:\n","                new_fname = os.path.basename(p)\n","                wrapped = gt\n","                if not (wrapped.startswith(f\"<{TASK_TAG}>\") and wrapped.endswith(f\"</{TASK_TAG}>\")):\n","                    wrapped = f\"<{TASK_TAG}>{wrapped}</{TASK_TAG}>\"\n","                records.append({\"image\": new_fname, \"image_path\": p, \"ground_truth\": wrapped})\n","        elif lower.endswith((\".png\", \".jpg\", \".jpeg\",\".tiff\")):\n","            dst = os.path.join(processed_images_dir, fname)\n","            shutil.copy(src_path, dst)\n","            wrapped = gt\n","            if not (wrapped.startswith(f\"<{TASK_TAG}>\") and wrapped.endswith(f\"</{TASK_TAG}>\")):\n","                wrapped = f\"<{TASK_TAG}>{wrapped}</{TASK_TAG}>\"\n","            records.append({\"image\": fname, \"image_path\": dst, \"ground_truth\": wrapped})\n","        else:\n","            print(f\"⚠️ Unsupported file type: {src_path}. Skipping.\")\n","    return records"],"metadata":{"id":"PNSnJA891XjJ","executionInfo":{"status":"ok","timestamp":1762530749308,"user_tz":-330,"elapsed":4,"user":{"displayName":"Prathamesh Shanbhag","userId":"16977587014656307330"}}},"execution_count":11,"outputs":[]},{"cell_type":"code","source":["# ---------------- Prepare processed dataset (pdf->png) ----------------\n","TMP_PROC = \"/tmp/donut_proc\"\n","if os.path.exists(TMP_PROC):\n","    shutil.rmtree(TMP_PROC)\n","ensure_dir(TMP_PROC)\n","\n","print(\"Preparing dataset (converting PDFs to PNGs where needed)...\")\n","train_records = prepare_files_and_records(DATASET_ROOT, \"train\", TMP_PROC)\n","val_records   = prepare_files_and_records(DATASET_ROOT, \"val\", TMP_PROC)\n","print(f\"Train examples (pages/images): {len(train_records)}\")\n","print(f\"Val examples (pages/images): {len(val_records)}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"iHt1TmYq6Pwk","executionInfo":{"status":"ok","timestamp":1762530890200,"user_tz":-330,"elapsed":140888,"user":{"displayName":"Prathamesh Shanbhag","userId":"16977587014656307330"}},"outputId":"a7a47460-f067-4dd8-f157-98f3fbf003a7"},"execution_count":12,"outputs":[{"output_type":"stream","name":"stdout","text":["Preparing dataset (converting PDFs to PNGs where needed)...\n","Train examples (pages/images): 228\n","Val examples (pages/images): 34\n"]}]},{"cell_type":"code","source":["# ---------------- Load processor & model ----------------\n","print(\"Loading processor and model:\", MODEL_NAME)\n","processor = AutoProcessor.from_pretrained(MODEL_NAME)\n","model = VisionEncoderDecoderModel.from_pretrained(MODEL_NAME)\n","\n","# Ensure tokenizer exists\n","if not hasattr(processor, \"tokenizer\"):\n","    raise RuntimeError(\"Processor does not contain tokenizer; cannot proceed.\")\n","\n","tokenizer = processor.tokenizer\n","\n","# model config tweaks\n","model.config.max_length = MAX_TARGET_LENGTH\n","model.config.decoder_start_token_id = tokenizer.cls_token_id or tokenizer.bos_token_id\n","model.config.pad_token_id = tokenizer.pad_token_id\n","model.config.eos_token_id = tokenizer.sep_token_id or tokenizer.eos_token_id"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1HrAPRBW6SPu","executionInfo":{"status":"ok","timestamp":1762530901235,"user_tz":-330,"elapsed":11023,"user":{"displayName":"Prathamesh Shanbhag","userId":"16977587014656307330"}},"outputId":"05fa31ac-48d3-4a55-8093-036cda620198"},"execution_count":13,"outputs":[{"output_type":"stream","name":"stdout","text":["Loading processor and model: naver-clova-ix/donut-base-finetuned-cord-v2\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.12/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n","  warnings.warn(\n"]}]},{"cell_type":"code","source":["# Optionally shuffle training records\n","random.seed(SEED)\n","random.shuffle(train_records)\n","\n","# Build HuggingFace Datasets\n","train_ds = Dataset.from_list(train_records)\n","val_ds = Dataset.from_list(val_records)"],"metadata":{"id":"q7YzU5oh8tEN","executionInfo":{"status":"ok","timestamp":1762530901288,"user_tz":-330,"elapsed":10,"user":{"displayName":"Prathamesh Shanbhag","userId":"16977587014656307330"}}},"execution_count":14,"outputs":[]},{"cell_type":"code","source":["def safe_load_image(path: str):\n","    try:\n","        im = Image.open(path).convert(\"RGB\")\n","        im.verify()  # check for integrity\n","        # reopen because verify() closes the file handle\n","        im = Image.open(path).convert(\"RGB\")\n","        return im\n","    except Exception as e:\n","        print(f\"⚠️ Skipping bad image: {path} ({e})\")\n","        return None"],"metadata":{"id":"vDt9asJT8prj","executionInfo":{"status":"ok","timestamp":1762530901304,"user_tz":-330,"elapsed":8,"user":{"displayName":"Prathamesh Shanbhag","userId":"16977587014656307330"}}},"execution_count":15,"outputs":[]},{"cell_type":"code","source":["def preprocess_batch(examples):\n","    \"\"\"\n","    Robust preprocessing for Donut fine-tuning.\n","    - Safely loads each image (PNG/JPG)\n","    - Skips unreadable/corrupted files instead of crashing\n","    - Returns pixel_values + tokenized labels\n","    \"\"\"\n","    images = []\n","    valid_ground_truths = []\n","\n","    # Safely load each image\n","    for img_path, gt in zip(examples[\"image_path\"], examples[\"ground_truth\"]):\n","        im = safe_load_image(img_path)\n","        if im is not None:\n","            images.append(im)\n","            valid_ground_truths.append(gt)\n","        else:\n","            # skip this example if image is unreadable\n","            continue\n","\n","    if not images:\n","        # If every image in the batch failed, return an empty batch safely\n","        return {\"pixel_values\": [], \"labels\": []}\n","\n","    # Process images -> pixel values\n","    encodings = processor(images=images, return_tensors=\"pt\")\n","\n","    # Tokenize target JSON text\n","    with tokenizer.as_target_tokenizer():\n","        labels = tokenizer(\n","            valid_ground_truths,\n","            padding=\"max_length\",\n","            truncation=True,\n","            max_length=MAX_TARGET_LENGTH,\n","        )\n","\n","    # Convert torch tensors to lists (datasets.map expects list of lists)\n","    out = {\n","        \"pixel_values\": [pv.tolist() for pv in encodings[\"pixel_values\"]],\n","        \"labels\": labels[\"input_ids\"],\n","        \"image\": [os.path.basename(p) for p in examples[\"image_path\"][:len(images)]],\n","        \"image_path\": examples[\"image_path\"][:len(images)],\n","    }\n","\n","    return out\n","\n","print(\"Mapping preprocess (this may take a little while)...\")\n","train_ds = train_ds.map(preprocess_batch, batched=True, batch_size=4, remove_columns=train_ds.column_names)\n","val_ds = val_ds.map(preprocess_batch, batched=True, batch_size=4, remove_columns=val_ds.column_names)\n","\n","# Set format to torch for keys that will be used\n","def convert_to_torch_format(ds):\n","    # keep `pixel_values` and `labels` as lists; we'll collate later\n","    return ds\n","\n","train_ds = convert_to_torch_format(train_ds)\n","val_ds = convert_to_torch_format(val_ds)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":182,"referenced_widgets":["b53508245213403982ef496277ff5d15","7459b6e398d14e4489f212bf8341e2c8","f8cdd1e058d44be2ab31cf6744cd7fd5","e939e41aef3047129e9c9824f839583e","2256da619d81435ea02f95f2c70f7011","d7e539ef08d64c9088fe95a070ebf2e2","c1b9698374754135a9c1edd125d00a82","002829287e4b44499a0c8f32a7b38733","1479fe0029e14bb382edc0b8853a0fc4","d7961f2c52594affa9ce457c3624c9b9","26645bc2769b486aa27e202e36d45ccb","7686ce04e2bc4866a17b283106ca6b6e","84fe58b7802b434ba386e21e7e70922f","6b21e2fbebe34fe3b243579d6bb02023","5e259548621d42d4a7a64d20d87ddfc2","a2818bdd05b6459db14e678d26631a8d","8e3258298a284518a83aaf61791dd402","69f03bced9ec4c1da3d627f0590556e7","b47d25270958431489b11504a7b85676","d588172e592446929b7e1b9c082d31dd","0807168686fb440aa82d513a404cbe80","0ca78265e49a4268a5dd21ab571ce215"]},"id":"x35E2qe86i7K","executionInfo":{"status":"ok","timestamp":1762531098225,"user_tz":-330,"elapsed":196912,"user":{"displayName":"Prathamesh Shanbhag","userId":"16977587014656307330"}},"outputId":"88f57521-cdc6-4a76-c78a-414d567f116e"},"execution_count":16,"outputs":[{"output_type":"stream","name":"stdout","text":["Mapping preprocess (this may take a little while)...\n"]},{"output_type":"display_data","data":{"text/plain":["Map:   0%|          | 0/228 [00:00<?, ? examples/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b53508245213403982ef496277ff5d15"}},"metadata":{}},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.12/dist-packages/transformers/tokenization_utils_base.py:4126: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["⚠️ Skipping bad image: /tmp/donut_proc/train/images/Manpower bill.tiff (decoder error -2)\n"]},{"output_type":"display_data","data":{"text/plain":["Map:   0%|          | 0/34 [00:00<?, ? examples/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7686ce04e2bc4866a17b283106ca6b6e"}},"metadata":{}}]},{"cell_type":"code","source":["# ---------------- Apply LoRA with PEFT ----------------\n","print(\"Freezing encoder parameters...\")\n","for p in model.encoder.parameters():\n","    p.requires_grad = False\n","\n","print(\"Configuring LoRA...\")\n","lora_config = LoraConfig(\n","    r=LORA_R,\n","    lora_alpha=LORA_ALPHA,\n","    target_modules=[\"q_proj\", \"v_proj\", \"k_proj\", \"o_proj\", \"dense\", \"fc_out\"],\n","    lora_dropout=LORA_DROPOUT,\n","    bias=\"none\",\n","    #task_type=TaskType.SEQ_2_SEQ_LM,\n",")\n","\n","model = get_peft_model(model, lora_config)\n","model.print_trainable_parameters()\n","model.to(device)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"oCx_Hl226k7l","executionInfo":{"status":"ok","timestamp":1762531102026,"user_tz":-330,"elapsed":3757,"user":{"displayName":"Prathamesh Shanbhag","userId":"16977587014656307330"}},"outputId":"08b40aeb-c5ae-4b6c-a97d-4cefbe809c85"},"execution_count":17,"outputs":[{"output_type":"stream","name":"stdout","text":["Freezing encoder parameters...\n","Configuring LoRA...\n","trainable params: 1,351,680 || all params: 202,473,592 || trainable%: 0.6676\n"]},{"output_type":"execute_result","data":{"text/plain":["PeftModel(\n","  (base_model): LoraModel(\n","    (model): VisionEncoderDecoderModel(\n","      (encoder): DonutSwinModel(\n","        (embeddings): DonutSwinEmbeddings(\n","          (patch_embeddings): DonutSwinPatchEmbeddings(\n","            (projection): Conv2d(3, 128, kernel_size=(4, 4), stride=(4, 4))\n","          )\n","          (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n","          (dropout): Dropout(p=0.0, inplace=False)\n","        )\n","        (encoder): DonutSwinEncoder(\n","          (layers): ModuleList(\n","            (0): DonutSwinStage(\n","              (blocks): ModuleList(\n","                (0-1): 2 x DonutSwinLayer(\n","                  (layernorm_before): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n","                  (attention): DonutSwinAttention(\n","                    (self): DonutSwinSelfAttention(\n","                      (query): Linear(in_features=128, out_features=128, bias=True)\n","                      (key): Linear(in_features=128, out_features=128, bias=True)\n","                      (value): Linear(in_features=128, out_features=128, bias=True)\n","                      (dropout): Dropout(p=0.0, inplace=False)\n","                    )\n","                    (output): DonutSwinSelfOutput(\n","                      (dense): lora.Linear(\n","                        (base_layer): Linear(in_features=128, out_features=128, bias=True)\n","                        (lora_dropout): ModuleDict(\n","                          (default): Dropout(p=0.05, inplace=False)\n","                        )\n","                        (lora_A): ModuleDict(\n","                          (default): Linear(in_features=128, out_features=8, bias=False)\n","                        )\n","                        (lora_B): ModuleDict(\n","                          (default): Linear(in_features=8, out_features=128, bias=False)\n","                        )\n","                        (lora_embedding_A): ParameterDict()\n","                        (lora_embedding_B): ParameterDict()\n","                        (lora_magnitude_vector): ModuleDict()\n","                      )\n","                      (dropout): Dropout(p=0.0, inplace=False)\n","                    )\n","                  )\n","                  (drop_path): DonutSwinDropPath(p=0.1)\n","                  (layernorm_after): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n","                  (intermediate): DonutSwinIntermediate(\n","                    (dense): lora.Linear(\n","                      (base_layer): Linear(in_features=128, out_features=512, bias=True)\n","                      (lora_dropout): ModuleDict(\n","                        (default): Dropout(p=0.05, inplace=False)\n","                      )\n","                      (lora_A): ModuleDict(\n","                        (default): Linear(in_features=128, out_features=8, bias=False)\n","                      )\n","                      (lora_B): ModuleDict(\n","                        (default): Linear(in_features=8, out_features=512, bias=False)\n","                      )\n","                      (lora_embedding_A): ParameterDict()\n","                      (lora_embedding_B): ParameterDict()\n","                      (lora_magnitude_vector): ModuleDict()\n","                    )\n","                    (intermediate_act_fn): GELUActivation()\n","                  )\n","                  (output): DonutSwinOutput(\n","                    (dense): lora.Linear(\n","                      (base_layer): Linear(in_features=512, out_features=128, bias=True)\n","                      (lora_dropout): ModuleDict(\n","                        (default): Dropout(p=0.05, inplace=False)\n","                      )\n","                      (lora_A): ModuleDict(\n","                        (default): Linear(in_features=512, out_features=8, bias=False)\n","                      )\n","                      (lora_B): ModuleDict(\n","                        (default): Linear(in_features=8, out_features=128, bias=False)\n","                      )\n","                      (lora_embedding_A): ParameterDict()\n","                      (lora_embedding_B): ParameterDict()\n","                      (lora_magnitude_vector): ModuleDict()\n","                    )\n","                    (dropout): Dropout(p=0.0, inplace=False)\n","                  )\n","                )\n","              )\n","              (downsample): DonutSwinPatchMerging(\n","                (reduction): Linear(in_features=512, out_features=256, bias=False)\n","                (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n","              )\n","            )\n","            (1): DonutSwinStage(\n","              (blocks): ModuleList(\n","                (0-1): 2 x DonutSwinLayer(\n","                  (layernorm_before): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n","                  (attention): DonutSwinAttention(\n","                    (self): DonutSwinSelfAttention(\n","                      (query): Linear(in_features=256, out_features=256, bias=True)\n","                      (key): Linear(in_features=256, out_features=256, bias=True)\n","                      (value): Linear(in_features=256, out_features=256, bias=True)\n","                      (dropout): Dropout(p=0.0, inplace=False)\n","                    )\n","                    (output): DonutSwinSelfOutput(\n","                      (dense): lora.Linear(\n","                        (base_layer): Linear(in_features=256, out_features=256, bias=True)\n","                        (lora_dropout): ModuleDict(\n","                          (default): Dropout(p=0.05, inplace=False)\n","                        )\n","                        (lora_A): ModuleDict(\n","                          (default): Linear(in_features=256, out_features=8, bias=False)\n","                        )\n","                        (lora_B): ModuleDict(\n","                          (default): Linear(in_features=8, out_features=256, bias=False)\n","                        )\n","                        (lora_embedding_A): ParameterDict()\n","                        (lora_embedding_B): ParameterDict()\n","                        (lora_magnitude_vector): ModuleDict()\n","                      )\n","                      (dropout): Dropout(p=0.0, inplace=False)\n","                    )\n","                  )\n","                  (drop_path): DonutSwinDropPath(p=0.1)\n","                  (layernorm_after): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n","                  (intermediate): DonutSwinIntermediate(\n","                    (dense): lora.Linear(\n","                      (base_layer): Linear(in_features=256, out_features=1024, bias=True)\n","                      (lora_dropout): ModuleDict(\n","                        (default): Dropout(p=0.05, inplace=False)\n","                      )\n","                      (lora_A): ModuleDict(\n","                        (default): Linear(in_features=256, out_features=8, bias=False)\n","                      )\n","                      (lora_B): ModuleDict(\n","                        (default): Linear(in_features=8, out_features=1024, bias=False)\n","                      )\n","                      (lora_embedding_A): ParameterDict()\n","                      (lora_embedding_B): ParameterDict()\n","                      (lora_magnitude_vector): ModuleDict()\n","                    )\n","                    (intermediate_act_fn): GELUActivation()\n","                  )\n","                  (output): DonutSwinOutput(\n","                    (dense): lora.Linear(\n","                      (base_layer): Linear(in_features=1024, out_features=256, bias=True)\n","                      (lora_dropout): ModuleDict(\n","                        (default): Dropout(p=0.05, inplace=False)\n","                      )\n","                      (lora_A): ModuleDict(\n","                        (default): Linear(in_features=1024, out_features=8, bias=False)\n","                      )\n","                      (lora_B): ModuleDict(\n","                        (default): Linear(in_features=8, out_features=256, bias=False)\n","                      )\n","                      (lora_embedding_A): ParameterDict()\n","                      (lora_embedding_B): ParameterDict()\n","                      (lora_magnitude_vector): ModuleDict()\n","                    )\n","                    (dropout): Dropout(p=0.0, inplace=False)\n","                  )\n","                )\n","              )\n","              (downsample): DonutSwinPatchMerging(\n","                (reduction): Linear(in_features=1024, out_features=512, bias=False)\n","                (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","              )\n","            )\n","            (2): DonutSwinStage(\n","              (blocks): ModuleList(\n","                (0-13): 14 x DonutSwinLayer(\n","                  (layernorm_before): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n","                  (attention): DonutSwinAttention(\n","                    (self): DonutSwinSelfAttention(\n","                      (query): Linear(in_features=512, out_features=512, bias=True)\n","                      (key): Linear(in_features=512, out_features=512, bias=True)\n","                      (value): Linear(in_features=512, out_features=512, bias=True)\n","                      (dropout): Dropout(p=0.0, inplace=False)\n","                    )\n","                    (output): DonutSwinSelfOutput(\n","                      (dense): lora.Linear(\n","                        (base_layer): Linear(in_features=512, out_features=512, bias=True)\n","                        (lora_dropout): ModuleDict(\n","                          (default): Dropout(p=0.05, inplace=False)\n","                        )\n","                        (lora_A): ModuleDict(\n","                          (default): Linear(in_features=512, out_features=8, bias=False)\n","                        )\n","                        (lora_B): ModuleDict(\n","                          (default): Linear(in_features=8, out_features=512, bias=False)\n","                        )\n","                        (lora_embedding_A): ParameterDict()\n","                        (lora_embedding_B): ParameterDict()\n","                        (lora_magnitude_vector): ModuleDict()\n","                      )\n","                      (dropout): Dropout(p=0.0, inplace=False)\n","                    )\n","                  )\n","                  (drop_path): DonutSwinDropPath(p=0.1)\n","                  (layernorm_after): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n","                  (intermediate): DonutSwinIntermediate(\n","                    (dense): lora.Linear(\n","                      (base_layer): Linear(in_features=512, out_features=2048, bias=True)\n","                      (lora_dropout): ModuleDict(\n","                        (default): Dropout(p=0.05, inplace=False)\n","                      )\n","                      (lora_A): ModuleDict(\n","                        (default): Linear(in_features=512, out_features=8, bias=False)\n","                      )\n","                      (lora_B): ModuleDict(\n","                        (default): Linear(in_features=8, out_features=2048, bias=False)\n","                      )\n","                      (lora_embedding_A): ParameterDict()\n","                      (lora_embedding_B): ParameterDict()\n","                      (lora_magnitude_vector): ModuleDict()\n","                    )\n","                    (intermediate_act_fn): GELUActivation()\n","                  )\n","                  (output): DonutSwinOutput(\n","                    (dense): lora.Linear(\n","                      (base_layer): Linear(in_features=2048, out_features=512, bias=True)\n","                      (lora_dropout): ModuleDict(\n","                        (default): Dropout(p=0.05, inplace=False)\n","                      )\n","                      (lora_A): ModuleDict(\n","                        (default): Linear(in_features=2048, out_features=8, bias=False)\n","                      )\n","                      (lora_B): ModuleDict(\n","                        (default): Linear(in_features=8, out_features=512, bias=False)\n","                      )\n","                      (lora_embedding_A): ParameterDict()\n","                      (lora_embedding_B): ParameterDict()\n","                      (lora_magnitude_vector): ModuleDict()\n","                    )\n","                    (dropout): Dropout(p=0.0, inplace=False)\n","                  )\n","                )\n","              )\n","              (downsample): DonutSwinPatchMerging(\n","                (reduction): Linear(in_features=2048, out_features=1024, bias=False)\n","                (norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n","              )\n","            )\n","            (3): DonutSwinStage(\n","              (blocks): ModuleList(\n","                (0-1): 2 x DonutSwinLayer(\n","                  (layernorm_before): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","                  (attention): DonutSwinAttention(\n","                    (self): DonutSwinSelfAttention(\n","                      (query): Linear(in_features=1024, out_features=1024, bias=True)\n","                      (key): Linear(in_features=1024, out_features=1024, bias=True)\n","                      (value): Linear(in_features=1024, out_features=1024, bias=True)\n","                      (dropout): Dropout(p=0.0, inplace=False)\n","                    )\n","                    (output): DonutSwinSelfOutput(\n","                      (dense): lora.Linear(\n","                        (base_layer): Linear(in_features=1024, out_features=1024, bias=True)\n","                        (lora_dropout): ModuleDict(\n","                          (default): Dropout(p=0.05, inplace=False)\n","                        )\n","                        (lora_A): ModuleDict(\n","                          (default): Linear(in_features=1024, out_features=8, bias=False)\n","                        )\n","                        (lora_B): ModuleDict(\n","                          (default): Linear(in_features=8, out_features=1024, bias=False)\n","                        )\n","                        (lora_embedding_A): ParameterDict()\n","                        (lora_embedding_B): ParameterDict()\n","                        (lora_magnitude_vector): ModuleDict()\n","                      )\n","                      (dropout): Dropout(p=0.0, inplace=False)\n","                    )\n","                  )\n","                  (drop_path): DonutSwinDropPath(p=0.1)\n","                  (layernorm_after): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","                  (intermediate): DonutSwinIntermediate(\n","                    (dense): lora.Linear(\n","                      (base_layer): Linear(in_features=1024, out_features=4096, bias=True)\n","                      (lora_dropout): ModuleDict(\n","                        (default): Dropout(p=0.05, inplace=False)\n","                      )\n","                      (lora_A): ModuleDict(\n","                        (default): Linear(in_features=1024, out_features=8, bias=False)\n","                      )\n","                      (lora_B): ModuleDict(\n","                        (default): Linear(in_features=8, out_features=4096, bias=False)\n","                      )\n","                      (lora_embedding_A): ParameterDict()\n","                      (lora_embedding_B): ParameterDict()\n","                      (lora_magnitude_vector): ModuleDict()\n","                    )\n","                    (intermediate_act_fn): GELUActivation()\n","                  )\n","                  (output): DonutSwinOutput(\n","                    (dense): lora.Linear(\n","                      (base_layer): Linear(in_features=4096, out_features=1024, bias=True)\n","                      (lora_dropout): ModuleDict(\n","                        (default): Dropout(p=0.05, inplace=False)\n","                      )\n","                      (lora_A): ModuleDict(\n","                        (default): Linear(in_features=4096, out_features=8, bias=False)\n","                      )\n","                      (lora_B): ModuleDict(\n","                        (default): Linear(in_features=8, out_features=1024, bias=False)\n","                      )\n","                      (lora_embedding_A): ParameterDict()\n","                      (lora_embedding_B): ParameterDict()\n","                      (lora_magnitude_vector): ModuleDict()\n","                    )\n","                    (dropout): Dropout(p=0.0, inplace=False)\n","                  )\n","                )\n","              )\n","            )\n","          )\n","        )\n","        (pooler): AdaptiveAvgPool1d(output_size=1)\n","      )\n","      (decoder): MBartForCausalLM(\n","        (model): MBartDecoderWrapper(\n","          (decoder): MBartDecoder(\n","            (embed_tokens): MBartScaledWordEmbedding(57580, 1024, padding_idx=1)\n","            (embed_positions): MBartLearnedPositionalEmbedding(770, 1024)\n","            (layers): ModuleList(\n","              (0-3): 4 x MBartDecoderLayer(\n","                (self_attn): MBartAttention(\n","                  (k_proj): lora.Linear(\n","                    (base_layer): Linear(in_features=1024, out_features=1024, bias=True)\n","                    (lora_dropout): ModuleDict(\n","                      (default): Dropout(p=0.05, inplace=False)\n","                    )\n","                    (lora_A): ModuleDict(\n","                      (default): Linear(in_features=1024, out_features=8, bias=False)\n","                    )\n","                    (lora_B): ModuleDict(\n","                      (default): Linear(in_features=8, out_features=1024, bias=False)\n","                    )\n","                    (lora_embedding_A): ParameterDict()\n","                    (lora_embedding_B): ParameterDict()\n","                    (lora_magnitude_vector): ModuleDict()\n","                  )\n","                  (v_proj): lora.Linear(\n","                    (base_layer): Linear(in_features=1024, out_features=1024, bias=True)\n","                    (lora_dropout): ModuleDict(\n","                      (default): Dropout(p=0.05, inplace=False)\n","                    )\n","                    (lora_A): ModuleDict(\n","                      (default): Linear(in_features=1024, out_features=8, bias=False)\n","                    )\n","                    (lora_B): ModuleDict(\n","                      (default): Linear(in_features=8, out_features=1024, bias=False)\n","                    )\n","                    (lora_embedding_A): ParameterDict()\n","                    (lora_embedding_B): ParameterDict()\n","                    (lora_magnitude_vector): ModuleDict()\n","                  )\n","                  (q_proj): lora.Linear(\n","                    (base_layer): Linear(in_features=1024, out_features=1024, bias=True)\n","                    (lora_dropout): ModuleDict(\n","                      (default): Dropout(p=0.05, inplace=False)\n","                    )\n","                    (lora_A): ModuleDict(\n","                      (default): Linear(in_features=1024, out_features=8, bias=False)\n","                    )\n","                    (lora_B): ModuleDict(\n","                      (default): Linear(in_features=8, out_features=1024, bias=False)\n","                    )\n","                    (lora_embedding_A): ParameterDict()\n","                    (lora_embedding_B): ParameterDict()\n","                    (lora_magnitude_vector): ModuleDict()\n","                  )\n","                  (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n","                )\n","                (activation_fn): GELUActivation()\n","                (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","                (encoder_attn): MBartAttention(\n","                  (k_proj): lora.Linear(\n","                    (base_layer): Linear(in_features=1024, out_features=1024, bias=True)\n","                    (lora_dropout): ModuleDict(\n","                      (default): Dropout(p=0.05, inplace=False)\n","                    )\n","                    (lora_A): ModuleDict(\n","                      (default): Linear(in_features=1024, out_features=8, bias=False)\n","                    )\n","                    (lora_B): ModuleDict(\n","                      (default): Linear(in_features=8, out_features=1024, bias=False)\n","                    )\n","                    (lora_embedding_A): ParameterDict()\n","                    (lora_embedding_B): ParameterDict()\n","                    (lora_magnitude_vector): ModuleDict()\n","                  )\n","                  (v_proj): lora.Linear(\n","                    (base_layer): Linear(in_features=1024, out_features=1024, bias=True)\n","                    (lora_dropout): ModuleDict(\n","                      (default): Dropout(p=0.05, inplace=False)\n","                    )\n","                    (lora_A): ModuleDict(\n","                      (default): Linear(in_features=1024, out_features=8, bias=False)\n","                    )\n","                    (lora_B): ModuleDict(\n","                      (default): Linear(in_features=8, out_features=1024, bias=False)\n","                    )\n","                    (lora_embedding_A): ParameterDict()\n","                    (lora_embedding_B): ParameterDict()\n","                    (lora_magnitude_vector): ModuleDict()\n","                  )\n","                  (q_proj): lora.Linear(\n","                    (base_layer): Linear(in_features=1024, out_features=1024, bias=True)\n","                    (lora_dropout): ModuleDict(\n","                      (default): Dropout(p=0.05, inplace=False)\n","                    )\n","                    (lora_A): ModuleDict(\n","                      (default): Linear(in_features=1024, out_features=8, bias=False)\n","                    )\n","                    (lora_B): ModuleDict(\n","                      (default): Linear(in_features=8, out_features=1024, bias=False)\n","                    )\n","                    (lora_embedding_A): ParameterDict()\n","                    (lora_embedding_B): ParameterDict()\n","                    (lora_magnitude_vector): ModuleDict()\n","                  )\n","                  (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n","                )\n","                (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","                (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n","                (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n","                (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","              )\n","            )\n","            (layernorm_embedding): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","            (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","          )\n","        )\n","        (lm_head): Linear(in_features=1024, out_features=57580, bias=False)\n","      )\n","    )\n","  )\n",")"]},"metadata":{},"execution_count":17}]},{"cell_type":"code","source":["def collate_fn(batch):\n","    # batch: list of examples where pixel_values is a nested list and labels is list\n","    pixel_values = [torch.tensor(x[\"pixel_values\"]) for x in batch]\n","    pixel_values = torch.stack([\n","        pv.squeeze(0) if pv.ndim == 4 and pv.shape[0] == 1 else pv\n","        for pv in pixel_values\n","    ])  # keep on CPU\n","\n","    labels = [torch.tensor(x[\"labels\"]) for x in batch]\n","    labels = torch.nn.utils.rnn.pad_sequence(\n","        labels,\n","        batch_first=True,\n","        padding_value=tokenizer.pad_token_id\n","    )  # keep on CPU\n","\n","    # DO NOT move to device here — Trainer will do it\n","    return {\"pixel_values\": pixel_values, \"labels\": labels}"],"metadata":{"id":"KKUcCtmu6ngJ","executionInfo":{"status":"ok","timestamp":1762531102043,"user_tz":-330,"elapsed":10,"user":{"displayName":"Prathamesh Shanbhag","userId":"16977587014656307330"}}},"execution_count":18,"outputs":[]},{"cell_type":"code","source":["# ---------------- TrainingArguments & Trainer ----------------\n","training_args = TrainingArguments(\n","    output_dir=OUTPUT_DIR,\n","    per_device_train_batch_size=TRAIN_BATCH_SIZE,\n","    per_device_eval_batch_size=EVAL_BATCH_SIZE,\n","    num_train_epochs=NUM_EPOCHS,\n","    learning_rate=LEARNING_RATE,\n","    logging_steps=LOGGING_STEPS,\n","    save_steps=SAVE_STEPS,\n","    save_total_limit=SAVE_TOTAL_LIMIT,\n","    evaluation_strategy=\"steps\" if len(val_ds) > 0 else \"no\",\n","    eval_steps=SAVE_STEPS if len(val_ds) > 0 else None,\n","    remove_unused_columns=False,\n","    fp16=torch.cuda.is_available(),\n","    push_to_hub=False,\n","    load_best_model_at_end=False,\n","    report_to=\"none\",\n",")\n","trainer = Trainer(\n","    model=model,\n","    args=training_args,\n","    train_dataset=train_ds,\n","    eval_dataset=val_ds if len(val_ds) > 0 else None,\n","    data_collator=collate_fn,\n",")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"xb34_yEp6rov","executionInfo":{"status":"ok","timestamp":1762531102226,"user_tz":-330,"elapsed":174,"user":{"displayName":"Prathamesh Shanbhag","userId":"16977587014656307330"}},"outputId":"e735faec-4778-41fc-e853-28170d8504fb"},"execution_count":19,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.12/dist-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n","  warnings.warn(\n"]}]},{"cell_type":"code","source":["# ---------------- Training run ----------------\n","if __name__ == \"__main__\":\n","    print(\"Starting training on device:\", device)\n","    trainer.train()\n","    print(\"Training complete — saving final model & processor...\")\n","    model.save_pretrained(OUTPUT_DIR)\n","    processor.save_pretrained(OUTPUT_DIR)\n","    print(\"Saved at:\", OUTPUT_DIR)\n","\n","    # simple inference helper\n","    def infer(image_path: str, max_length: int = MAX_TARGET_LENGTH):\n","        im = Image.open(image_path).convert(\"RGB\")\n","        inputs = processor(images=im, return_tensors=\"pt\").to(device)\n","        generate_kwargs = dict(max_length=max_length, num_beams=1)\n","        generated = model.generate(pixel_values=inputs[\"pixel_values\"].to(device), **generate_kwargs)\n","        out = tokenizer.batch_decode(generated, skip_special_tokens=True)[0]\n","        # out contains the <parse>...</parse> text\n","        return out\n","\n","    # quick test inference on a validation image\n","    if len(val_records) > 0:\n","        sample = val_records[0]\n","        print(\"Example inference on:\", sample[\"image_path\"])\n","        print(infer(sample[\"image_path\"]))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":857},"id":"KR4BrBa-6vvL","executionInfo":{"status":"error","timestamp":1762536336915,"user_tz":-330,"elapsed":5234467,"user":{"displayName":"Prathamesh Shanbhag","userId":"16977587014656307330"}},"outputId":"e8bb173b-f7ff-4e3a-dbb0-eae52e8030f5"},"execution_count":20,"outputs":[{"output_type":"stream","name":"stdout","text":["Starting training on device: cuda\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='802' max='2850' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [ 802/2850 1:26:50 < 3:42:19, 0.15 it/s, Epoch 7.03/25]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Step</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>100</td>\n","      <td>5.787100</td>\n","      <td>No log</td>\n","    </tr>\n","    <tr>\n","      <td>200</td>\n","      <td>4.983200</td>\n","      <td>No log</td>\n","    </tr>\n","    <tr>\n","      <td>300</td>\n","      <td>4.863900</td>\n","      <td>No log</td>\n","    </tr>\n","    <tr>\n","      <td>400</td>\n","      <td>3.962300</td>\n","      <td>No log</td>\n","    </tr>\n","    <tr>\n","      <td>500</td>\n","      <td>3.993300</td>\n","      <td>No log</td>\n","    </tr>\n","    <tr>\n","      <td>600</td>\n","      <td>3.616100</td>\n","      <td>No log</td>\n","    </tr>\n","    <tr>\n","      <td>700</td>\n","      <td>4.141300</td>\n","      <td>No log</td>\n","    </tr>\n","    <tr>\n","      <td>800</td>\n","      <td>3.568100</td>\n","      <td>No log</td>\n","    </tr>\n","  </tbody>\n","</table><p>"]},"metadata":{}},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m/tmp/ipython-input-310350926.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Starting training on device:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Training complete — saving final model & processor...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mOUTPUT_DIR\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1936\u001b[0m                 \u001b[0mhf_hub_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menable_progress_bars\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1937\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1938\u001b[0;31m             return inner_training_loop(\n\u001b[0m\u001b[1;32m   1939\u001b[0m                 \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1940\u001b[0m                 \u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36m_inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2234\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2235\u001b[0m             \u001b[0mstep\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2236\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch_iterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2237\u001b[0m                 \u001b[0mtotal_batched_samples\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2238\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/accelerate/data_loader.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    577\u001b[0m                     \u001b[0mcurrent_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msend_to_device\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcurrent_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_blocking\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_non_blocking\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    578\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_update_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 579\u001b[0;31m                 \u001b[0mnext_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataloader_iter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    580\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mbatch_index\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mskip_batches\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    581\u001b[0m                     \u001b[0;32myield\u001b[0m \u001b[0mcurrent_batch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    732\u001b[0m                 \u001b[0;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    733\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 734\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    735\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    736\u001b[0m             if (\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    788\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    789\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 790\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    791\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    792\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory_device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"__getitems__\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitems__\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitems__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/datasets/arrow_dataset.py\u001b[0m in \u001b[0;36m__getitems__\u001b[0;34m(self, keys)\u001b[0m\n\u001b[1;32m   2861\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__getitems__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeys\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2862\u001b[0m         \u001b[0;34m\"\"\"Can be used to get a batch using a list of integers indices.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2863\u001b[0;31m         \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitem__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2864\u001b[0m         \u001b[0mn_examples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2865\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mcol\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marray\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_examples\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/datasets/arrow_dataset.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   2857\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_format_type\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_format_type\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\"arrow\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"pandas\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"polars\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2858\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mColumn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2859\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2860\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2861\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__getitems__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeys\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/datasets/arrow_dataset.py\u001b[0m in \u001b[0;36m_getitem\u001b[0;34m(self, key, **kwargs)\u001b[0m\n\u001b[1;32m   2839\u001b[0m         \u001b[0mformatter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_formatter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mformat_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_info\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mformat_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2840\u001b[0m         \u001b[0mpa_subtable\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mquery_table\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_indices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2841\u001b[0;31m         formatted_output = format_table(\n\u001b[0m\u001b[1;32m   2842\u001b[0m             \u001b[0mpa_subtable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mformatter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mformatter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mformat_columns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mformat_columns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_all_columns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_all_columns\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2843\u001b[0m         )\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/datasets/formatting/formatting.py\u001b[0m in \u001b[0;36mformat_table\u001b[0;34m(table, key, formatter, format_columns, output_all_columns)\u001b[0m\n\u001b[1;32m    655\u001b[0m     \u001b[0mpython_formatter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPythonFormatter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mformatter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    656\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mformat_columns\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 657\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mformatter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpa_table\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquery_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mquery_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    658\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mquery_type\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"column\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    659\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mformat_columns\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/datasets/formatting/formatting.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, pa_table, query_type)\u001b[0m\n\u001b[1;32m    412\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat_column\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpa_table\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    413\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mquery_type\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"batch\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 414\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpa_table\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    415\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    416\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mformat_row\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpa_table\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mpa\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTable\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mRowFormat\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/datasets/formatting/formatting.py\u001b[0m in \u001b[0;36mformat_batch\u001b[0;34m(self, pa_table)\u001b[0m\n\u001b[1;32m    468\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlazy\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    469\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mLazyBatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpa_table\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 470\u001b[0;31m         \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython_arrow_extractor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextract_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpa_table\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    471\u001b[0m         \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython_features_decoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    472\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/datasets/formatting/formatting.py\u001b[0m in \u001b[0;36mextract_batch\u001b[0;34m(self, pa_table)\u001b[0m\n\u001b[1;32m    147\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextract_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpa_table\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mpa\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTable\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 149\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mpa_table\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_pydict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    150\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]}]}